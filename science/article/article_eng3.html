<!DOCTYPE html>
<html lang="ru">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Автоматизированный ремонт программ: новые тенденции создают и выявляют проблемы для бенчмарков - Синяева
        Элеонора Витальевна</title>
    <link rel="stylesheet" href="../../css/master_style.css">
    <link rel="stylesheet" href="../../css/article.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <script src="https://cdn.jsdelivr.net/npm/tsparticles@1.39.0/tsparticles.min.js"></script>
</head>

<body>

    <div id="tsparticles" style="position: fixed; top: 0; left: 0; width: 100%; height: 100%; z-index: -1;"></div>

    <!-- Верхний блок -->
    <div class="top-block">
        <div class="lang-box">
            <!-- <a href="article_eng3.html" class="lang-link">EN</a> -->
        </div>
        <div class="university-links">
            <a href="http://donntu.ru" target="_blank">ДонНТУ</a>
            <a href="http://masters.donntu.ru" target="_blank" title="Перейти на портал магистров ДонНТУ">Портал
                магистров</a>
        </div>
    </div>

    <!-- Основное содержимое -->
    <div class="container">
        <div class="article-container">
            <div class="article-content">
                

                <div class="article-header">
                    <div class="article-title">
                        АВТОМАТИЗИРОВАННЫЙ РЕМОНТ ПРОГРАММ: НОВЫЕ ТЕНДЕНЦИИ СОЗДАЮТ И ВЫЯВЛЯЮТ ПРОБЛЕМЫ ДЛЯ БЕНЧМАРКОВ
                    </div>

                    <div class="authors">
                        Joseph Renzullo<sup>∗</sup>, Pemma Reiter<sup>∗</sup>, Westley Weimer<sup>1</sup>, and Stephanie
                        Forrest<sup>†</sup>
                    </div>

                    <div class="university">
                        Университет штата Аризона, США<br>
                        <sup>1</sup>Университет Мичигана, США
                    </div>

                    <div class="contact">
                        <sup>∗</sup>Оба автора внесли равный вклад в это исследование.<br>
                        <sup>†</sup>Также аффилирована с Институтом Санта-Фе
                    </div>

                    <div class="abstract">
                        <p><strong>Рензулло Дж., Рейтер П., Веймер В., Форрест С. Автоматизированный ремонт программ:
                                новые тенденции создают и выявляют проблемы для бенчмарков.</strong> Машинное обучение
                            (ML) сегодня пронизывает область автоматизированного ремонта программ (Automated Program
                            Repair, APR). Алгоритмы используют нейросетевой машинный перевод и большие языковые модели
                            (LLM) для генерации программных заплат и других задач. Однако между этими приложениями ML и
                            более ранними работами существуют важные различия. При оценке и сравнении необходимо
                            внимательно следить за тем, чтобы результаты были корректными и с высокой вероятностью
                            обобщались. Проблема в том, что наиболее популярные бенчмарки для оценки APR не
                            разрабатывались с учётом методов ML. Это особенно верно для LLM, обучающиеся на больших и
                            часто слабо документированных датасетах, которые могут включать задачи, на которых затем
                            проводится оценка моделей.</p>
                        <p class="keywords"><strong>Ключевые слова:</strong> автоматизированный ремонт программ,
                            машинное обучение, бенчмарки, качество заплат</p>
                    </div>

                    <div class="section">
                        <div class="section-title">Введение</div>
                        <p>Автоматизированный ремонт программ (Automated Program Repair, APR) — это подполе инженерии
                            программного обеспечения, которое стремится сократить или устранить прямое участие человека
                            в исправлении программных дефектов (багов). Было разработано множество техник: эволюционные
                            методы [60, 133], подходы с использованием шаблонных операторов мутации [71], семантические
                            методы вывода [79], нацеленные на «один источник» дефекта, а также методы для обработки
                            багов, затрагивающих несколько участков кода [100]. Всё чаще исследователи применяют к
                            задачам APR методы, основанные на ML (раздел 3), однако серьёзную озабоченность вызывает
                            утечка данных (раздел 4). Каждая новая техника — или модификация существующей — обычно
                            разрабатывается независимой исследовательской группой без опоры на общее формальное
                            определение APR. Одних бенчмарков недостаточно, чтобы стандартизировать оценку (раздел 5).
                        </p>

                        <p>В качестве мотивационных примеров рассмотрим некоторые несоответствия в опубликованной
                            литературе:</p>

                        <p><strong>Корректность.</strong> VFix [123] считает корректными такие заплаты, которые проходят
                            все тесты и семантически или синтаксически эквивалентны исходному исправлению разработчика,
                            в то время как VRepair [26] оценивает точность ремонта в терминах семантической
                            эквивалентности исходному исправлению, а SynFix [10] определяет корректность просто как
                            прохождение тестов. Каждое из этих определений разумно само по себе, но в совокупности их
                            различия затрудняют сравнение результатов.</p>

                        <p><strong>Локализация ошибок.</strong> Sequencer [27] предполагает идеальную локализацию ошибки
                            на уровне строки, тогда как iFixR [56] использует сгенерированные человеком отчёты о багах
                            для локализации; большинство более ранних оценок APR выполняют локализацию самостоятельно.
                        </p>

                        <p><strong>Критерии завершения.</strong> TBar [71], Angelix [79] и многие другие методы
                            завершают работу после нахождения одного возможного решения, в то время как Dlfix [68]
                            прекращает поиск при исчерпании выделенного фиксированного временного бюджета.</p>

                        <p>Это лишь несколько примеров существенных различий в том, как разные авторы сообщают
                            результаты оценки на общих бенчмарках. В данной статье мы даём тематический обзор последних
                            разработок в использовании ML для задач APR. После краткого введения в используемую
                            терминологию (раздел 2) мы рассматриваем рост разнообразных ML‑подходов, применяемых к APR
                            (раздел 3), конкретные проблемы во взаимодействии обучающих и тестовых данных (раздел 4) и
                            роль бенчмарков в опосредовании и выявлении этих вопросов (раздел 5). Мы обсуждаем новый
                            конкурс APR и недавнее предложение по стандартизации применения ML‑моделей к задачам APR
                            (раздел 6), а затем подводим итоги.</p>

                        <div class="subsection">
                            <div class="section-title">Методология</div>
                            <p>Опубликовано несколько высококачественных общих обзоров области APR [39, 61, 83]; здесь
                                мы подходим уже — рассматриваем все статьи, опубликованные на пяти ведущих конференциях
                                по инженерии программного обеспечения с 2018 года, и фокусируемся на ML в APR. Это
                                позволяет проанализировать новые тенденции в наиболее популярных для APR‑исследователей
                                площадках.</p>

                            <p>Методологически мы анализировали «The Living Review of Automated Program Repair»
                                Монперрюса (hal-01956501, версия 5). Поскольку этот «живой обзор» регулярно обновляется,
                                он охватывает широкий спектр тем APR, включая домены ремонта и типы ошибок,
                                вспомогательные методы улучшения и исследования с участием людей. Используя этот обзор
                                как отправную точку, мы определили пять площадок, публикующих больше всего APR‑статей в
                                Living Review: ICSE (International Conference on Software Engineering), TSE
                                (Transactions on Software Engineering), FSE (Foundations of Software Engineering), ASE
                                (Automated Software Engineering) и EMSE (Empirical Software Engineering).</p>

                            <p>Затем мы независимо рассмотрели статьи, опубликованные в этих пяти изданиях с 1 января
                                2018 г. по 1 сентября 2023 г. Мы выделили первичные работы для нашего исследования по
                                метаданным статей, используя ключевые слова «patch» и «repair». Далее мы искали
                                потенциально связанные работы по вторичным ключевым словам: automate, fix, fault, bug,
                                vulnerability и generate. Статьи, содержащие любой из этих терминов (или их
                                основы/варианты), подвергались ручному анализу аннотации на предмет APR‑тематики. Из 118
                                статей, отобранных нашей методологией, лишь 65 перекрываются с обзором Монперрюса.</p>
                        </div>
                    </div>

                    <div class="section">
                        <div class="section-title">Фон</div>

                        <div class="subsection">
                            <div class="section-title">Трансформеры и механизм внимания</div>
                            <p>Изначально разработанные для обработки естественного языка (NLP), модели нейросетевого
                                машинного перевода (NMT) используют нейронные сети для задач перевода — таких как
                                предсказание предложений и перевод между языками [29]. Современные NMT‑модели обычно
                                реализуются как архитектуры «энкодер–декодер» на базе трансформеров с вниманием [75,
                                111], что является развитием более ранних архитектур «энкодер–декодер» на основе LSTM
                                [103] [29]. Архитектуры трансформеров разнообразны: используются варианты только‑энкодер
                                [32], только‑декодер [25] и пары «энкодер–декодер» [64, 97] в зависимости от задач.
                                Энкодер извлекает информацию и отображает её в скрытое пространство, тогда как декодер
                                генерирует содержимое, опираясь на представление входных данных в этом скрытом
                                пространстве.</p>

                            <p>Поскольку архитектуры только‑энкодер имеют ограниченные генеративные возможности в режиме
                                zero-shot‑обучения [106, 113], для генеративных задач часто применяются модели
                                только‑декодер и «энкодер–декодер». Хотя внимание не является специфичным исключительно
                                для трансформеров, оно является ключевой архитектурной особенностью, помогающей
                                масштабировать модели за счёт обработки входных последовательностей переменной длины
                                [22]. Механизм внимания позволяет выявлять связи между входом и выходом, когда позиции
                                важных признаков неизвестны. Это важно для APR‑задач, где входом обычно является
                                токенизированный исходный код, сильно варьирующийся по структуре и длине.</p>
                        </div>

                        <div class="subsection">
                            <div class="section-title">Языковые модели</div>
                            <p>Широко используемые в NLP, языковые модели предназначены для обучения контекстных
                                представлений текста и обычно реализуются как глубокие нейронные сети. Предобученная
                                языковая модель (Pre-trained Language Model, PLM) имеет параметры, инициализированные на
                                больших корпусах для получения универсальных текстовых представлений. Это позволяет
                                «амортизировать» значительную часть усилий, требуемых для обучения на больших датасетах,
                                и в дальнейшем дообучать модель под конкретные задачи [32, 106]. Большие языковые модели
                                (LLM) [136] — это, как следует из названия, масштабные языковые модели, обычно с
                                миллиардом и более параметров. LLM наследуют парадигмы предобучения и дообучения PLM, а
                                также их архитектуры. LLM быстро находят применение во многих областях, включая APR, где
                                их высокая производительность на сложных генеративных задачах является важным
                                конкурентным преимуществом.</p>
                        </div>

                        <div class="subsection">
                            <div class="section-title">Смещение данных</div>
                            <p>Смещение данных (data bias) — это класс ошибок ML, возникающих, когда признаки
                                представлены в датасете неравномерно, что приводит к искажённым результатам, ухудшению
                                качества и снижению точности [80]. По мере роста объёма обучающих данных выявлять
                                смещения и их возможные источники становится всё труднее. Непреднамеренное смещение
                                данных — распространённая проблема в задачах обработки естественного языка при
                                использовании реальных данных [12]. Однако то, как такие смещения проявляются в задачах
                                APR, изучено слабо. Например, модель NPR, использующая Codex, может быть смещена в
                                сторону человеко‑читаемых пар «вход–выход» и показывать низкую точность на
                                алгоритмически сгенерированном коде, поскольку Codex фильтрует подозрительный
                                авто‑сгенерированный код из обучающих корпусов [25]. Хотя этот и другие потенциальные
                                виды смещения, связанные с кодом, требуют дальнейшего изучения, исследователи нередко
                                смягчают известные смещения, дополняя обучающие данные синтетическими [47].
                                Синтетические данные генерируются так, чтобы сохранять релевантные статистические
                                свойства, при этом контролируя другие факторы. В рассмотренных нами статьях смещение
                                данных и методы его обработки явно не обсуждались.</p>
                        </div>

                        <div class="subsection">
                            <div class="section-title">Утечка данных</div>
                            <p>В ML утечка данных (data leakage) происходит, когда обучающие данные содержат информацию
                                или признаки, напрямую связанные с предсказываемой величиной. Это часто приводит к
                                завышенной оценке ожидаемой производительности модели на новых, ранее невиданных данных
                                [52]. Утечка может возникать, когда в обучающих данных присутствует сама предсказываемая
                                величина или её прокси, но также встречаются и другие формы утечки и «загрязнения»,
                                например наличие похожих (near-duplicate [8, 36, 57]) или идентичных (duplicate [63])
                                экземпляров. Исследователи ML обычно предпринимают серьёзные усилия, чтобы при
                                разделении данных на обучающую и тестовую выборки избежать дубликатов — то есть проводят
                                дедупликацию или деконтаминацию. Однако, как мы обсуждаем в разделе 4, этих методов
                                часто недостаточно. В языковых моделях дублирование может происходить на разных уровнях
                                гранулярности: предложения, документы, целые датасеты. Аналогично, в исходном коде
                                дубликаты возможны на уровне операторов, функций, классов и файлов.</p>
                        </div>

                        <div class="subsection">
                            <div class="section-title">Поиск с лучом (beam search)</div>
                            <p>В ML, в частности в задачах NLP, поиск с лучом (beam search) [105] управляет степенью
                                широты или исчерпывающей природы поиска. Это контролируется гиперпараметром размер луча
                                (beam size). На каждом шаге рассматриваются последовательности, и из множества
                                оставшихся выбираются последовательностей с наибольшими выходными весами.</p>
                        </div>
                    </div>

                    <div class="section">
                        <div class="section-title">Разнообразные ML‑подходы проникают в APR</div>
                        <p>В таблице 1 приведены ключевые слова, выбранные авторами 118 статей из нашего корпуса. Из них
                            29 % содержат ключевые слова, связанные с ML. В 2018 году это встречалось относительно редко
                            — менее чем в одной из шести статей. К 2023 году более трёх из четырёх опубликованных статей
                            использовали ML‑техники. Это поразительное изменение за короткий промежуток времени.</p>

                        <div class="subsection">
                            <div class="section-title">Подъём LLM</div>
                            <p>Увеличение размера моделей и объёма данных, на которых обучаются LLM, привело к появлению
                                ряда, по‑видимому, «эмерджентных» возможностей, включая пошаговое рассуждение и обучение
                                в контексте (in‑context learning) [114, 115]. Эти свойства вызывают интерес
                                исследователей [14] и широкой публики, поскольку воспринимаются как шаг к созданию
                                компонентов общего искусственного интеллекта (AGI). Однако сами эти свойства, факторы,
                                способствующие их появлению, и причины их отсутствия в более компактных моделях пока
                                недостаточно понятны [102]. Тем не менее LLM становятся популярным инструментом для
                                исследователей APR, в первую очередь для генерации заплат.</p>
                        </div>

                        <div class="subsection">
                            <div class="section-title">Языковые модели, специфичные для кода</div>
                            <p>Некоторые PLM и LLM включают исходный код в свои обучающие корпуса [25, 30, 67, 101],
                                зачастую получая его из открытых репозиториев вроде GitHub или площадок помощи по
                                программированию типа StackOverflow. Эти модели точнее решают задачи рассуждения, когда
                                задачи формулируются в виде, похожем на код [77]. Специализированные кодовые LLM,
                                дообученные на исходном коде, показали перспективные результаты в генеративных задачах,
                                таких как дополнение кода [25] и синтез программ [7]. Первые работы уже применяют LLM к
                                задаче APR [4, 34, 37, 50, 93, 119, 120, 129].</p>
                        </div>

                        <div class="subsection">
                            <div class="section-title">Архитектуры</div>
                            <p>Другие специализированные архитектуры ML также применяются к задачам, связанным с кодом.
                                Когда для APR используются нейросетевые техники, говорят о нейросетевом ремонте программ
                                (Neural Program Repair, NPR). Хотя для других задач с кодом использовались структурные
                                архитектуры, например, графовые нейронные сети для суммаризации кода [62], большинство
                                NPR‑архитектур опираются на NMT‑модели. NPR‑модели на базе NMT могут использовать
                                собственные реализации [110], но всё более популярными становятся кодовые LLM:
                                AlphaRepair [120] (CodeBERT [35]), CoditT5 [135] и RewardRepair [131] (CodeT5 [97]),
                                SelfAPR [129] (PLBART [3]).</p>

                            <p>В нашем обзоре мы обнаружили, что архитектура трансформера используется чаще любой другой
                                ML‑архитектуры. Из 29 статей, применяющих ML‑подходы, в 20 использовалась
                                модель‑трансформер, включая следующие 18 инструментов: RewardRepair [131], CURE [50],
                                DLFix [68], VRepair [26], SeqTrans [28], SynShine [4], SequenceR [27], AlphaRepair
                                [120], VulRepair [37], Recoder [138], Quatrain [109], SelfAPR [129], TransRepair [66],
                                Reptory [87], TENURE [82], Tare [139], Knod [49] и Rete [93]. Другие архитектуры НС
                                использовались как классификаторы — к примеру, многоклассовый BiLSTM‑классификатор в
                                TRANSFER [81]; как генеративные модели — RNN‑модель SynFix [10] и модель представления
                                кода на основе ASTNN в AccPR [124]; и, наконец, как модели для обучения признаков,
                                например, CNN, обучающаяся шаблонам исправлений [70].</p>

                            <p>Первая волна моделей, использующих ML для APR, в основном строилась на NMT, где парадигма
                                заключалась в переводе «багованный код → исправленный код». Текущая тенденция —
                                использовать LLM, где парадигма — удалить строку с ошибкой и использовать генеративные
                                текстовые модели для предсказания кода, заполняющего образовавшийся «пробел».</p>
                        </div>

                        <div class="subsection">
                            <div class="section-title">Механизмы внимания</div>
                            <p>Механизмы внимания в рассмотренных трансформер‑базированных инструментах реализованы
                                неединообразно: в одной работе он вообще не упоминается явно [138]; пять статей
                                описывают его весьма подробно [27, 28, 49, 68, 139]; остальные либо ссылаются на работы
                                по вниманию, такие как Vaswani [111] и Luong [75] [33, 50, 87, 93], либо дают краткое
                                описание [26, 66, 82], либо просто ссылаются на унаследованную NMT‑модель [4, 37, 108,
                                109, 120, 129, 131]. Хотя CURE [50] наследует архитектуру мульти‑хедового внимания от
                                используемой PLM GPT, авторы ссылаются и на классическую работу Васвани [111]. В
                                противоположность этому, SeqTrans [28] подробно описывает внимание, хотя и использует
                                OpenNMT, базирующийся на механизме внимания Луонга [75]. Такое различие в уровне
                                детализации может объясняться отсутствием общепринятой таксономии механизмов внимания
                                [22].</p>
                        </div>
                    </div>

                    <div class="section">
                        <div class="section-title">Утечка данных загрязняет модели и бенчмарки</div>

                        <div class="subsection">
                            <div class="section-title">Утечка данных</div>
                            <p>Утечка данных часто возникает, когда обучающие данные включают информацию, релевантную
                                задаче, но которую модель не должна иметь возможность «видеть». В рассмотренной
                                литературе, когда затрагивалась тема утечки, обычно применялось разделение train/test —
                                то есть чёткое разделение обучающего корпуса и тестового набора с минимизацией
                                пересечения контента. SelfAPR [129], ориентированный на проект‑специфичные знания с
                                помощью самосупервизированного обучения, делит обучающий и тестовый датасеты по
                                временной границе — времени коммита, исправляющего баг, в проекте. Такой подход также
                                использует CURE [50], обучающийся на данных до 2006 года, чтобы избежать загрязнения
                                бенчмарками (датируемыми 2007 годом и позже). Система Overfitting Detection [127] —
                                инструмент для оценки корректности APR‑заплат — применяет сегментацию по проектам: в
                                качестве обучающего набора используются Bugs.jar [99], Bears [78] и Defects4J [51], но
                                из тестового набора исключаются любые проекты (например, библиотека Apache Math),
                                присутствующие более чем в одном из этих источников, чтобы избежать дубликатов при
                                стандартном случайном разделении.</p>

                            <p>Для синтаксического обучения RewardRepair [131] использует уже существующие обучающие
                                корпуса пар «баг–исправление» и не выполняет какого‑либо разделения train/test или
                                деконтаминации этого контента. SynShine [4], применяющая предобученный трансформер
                                RoBERTa, выполняет стандартное разделение своего датасета из 1,7 млн пар «ошибочная
                                программа – исправленная программа», случайным образом выделяя 100 тыс. примеров для
                                тестирования.</p>
                        </div>

                        <div class="subsection">
                            <div class="section-title">Загрязнение</div>
                            <p>Загрязнение (contamination) представляет серьёзный риск для достоверности результатов,
                                полученных ML‑подходами к APR, поскольку ведёт к завышению ожидаемой производительности
                                на практике. Дублированные данные приводят к смещению модели в сторону отдельных
                                примеров и повышают вероятность того, что сгенерированное содержимое будет просто
                                запомнено [19]. В нашем обзоре мы не нашли APR‑статей, которые явно рассматривали бы
                                проблему near‑duplicates, хотя одна работа обсуждает возможное загрязнение PLM [119].
                                Деконтаминация может улучшать производительность LLM [17, 30], но она сложна. Во‑первых,
                                при наследовании предобученных весов пользователь не может выполнить дедупликацию
                                обучающего набора. Во‑вторых, детали обучения, включая сам обучающий корпус, могут быть
                                недоступны, и внешнему пользователю оказывается невозможно деконтаминировать тестовые
                                данные.</p>

                            <p>Мы обнаружили, что при использовании LLM для APR в пяти работах влияние утечки и
                                загрязнения данных вообще не учитывается [4, 34, 37, 93, 120]. Xia и др. [119] сообщают
                                информацию о загрязнении для каждой оцениваемой PLM с открытыми обучающими данными: они
                                перечисляют исправления, которые были идентичны изменениям разработчика и присутствовали
                                в обучающем наборе. SelfAPR [129], самостоятельно обучающая собственную LLM, удаляет все
                                дублирующиеся «багованные» примеры из обучающих корпусов. CURE [50], также переобучающая
                                LLM, формирует корпус из открытых Java‑проектов GitHub, откатывая их историю до 2006
                                года, а затем вручную удаляет дубликаты багов — это грубая дедупликация по методологии
                                CoCoNut [76]. Сравнивая политики деконтаминации SelfAPR и CURE, можно отметить, что CURE
                                предполагает независимость проектов GitHub и независимость между коммитами и
                                исправлениями (сильное допущение), тогда как SelfAPR анализирует каждый пример и
                                оставляет только уникальные. Три LLM‑метода для APR боролись с утечкой путём
                                дедупликации корпусов: один удалял дубликаты на основе синтаксиса [58], другой — на
                                основе семантики [109], третий не уточнял свой метод дедупликации [108].</p>
                        </div>

                        <div class="subsection">
                            <div class="section-title">Деконтаминация бенчмарков</div>
                            <p>Даже когда деконтаминация является явной целью, возможность её выполнить — или хотя бы
                                оценить степень загрязнения — часто отсутствует. Для предобученных моделей в задачах с
                                естественным языком, не говоря уже о задачах с кодом, ещё не сложился стандартный метод
                                деконтаминации. Например, GPT‑3 использовал N‑граммы для обнаружения загрязнения, где
                                «грамма» — это слово в нижнем регистре, разделённое пробелами и без пунктуации [13]. Для
                                GPT‑4 применяется фильтрация на основе многократного поиска подстрок [91]. Поскольку
                                обучающие корпуса этих моделей недоступны, сторонние пользователи не могут напрямую
                                применить политику деконтаминации OpenAI к новым бенчмаркам при использовании этих
                                моделей через API.</p>

                            <p>Кроме того, стандартные подходы к оценке вероятности того, что тестовый пример
                                присутствовал в обучающем наборе, например оценка разницы энтропий между «видимыми» и
                                «невидимыми» данными [104], неприменимы, когда сама модель недоступна (например, при
                                доступе только через API). Machine unlearning обновляет веса предобученной модели,
                                переобучая её или аппроксимируя эффект удаления определённых данных [15, 16, 118, 134].
                                Пока что исследования в области machine unlearning сосредоточены на вопросах
                                безопасности и приватности, но набирают обороты, особенно после объявления задачи
                                NeurIPS 2023 [1].</p>
                        </div>

                        <div class="subsection">
                            <div class="section-title">Запоминание и обобщение</div>
                            <p>Запоминание (memorization), одна из форм утечки данных, определяется как склонность
                                языковых моделей генерировать последовательности из обучающего набора буквально, не
                                усваивая их смысла [18]. В ряде задач с естественным языком, где требуется
                                воспроизводить факты, запоминание может быть полезным, но для задач с кодом его ценность
                                неоднозначна. Оно может помочь исправлять баги, идентичные уже увиденным, но создаёт
                                проблемы при проектировании моделей, от которых ожидается способность к обобщению. Хотя
                                полностью предотвратить запоминание конкретных обучающих примеров трудно, недавние
                                работы показывают, что удаление дубликатов и near‑duplicates из обучающих корпусов в
                                среднем снижает буквальное запоминание [63].</p>

                            <p>Немногие языковые модели описывают свою политику деконтаминации; ещё меньше моделей
                                предоставляют вместе с собой пакет деконтаминации для выявления загрязнённых тестовых
                                примеров и построения «чистой» версии бенчмарка (например, EleutherAI [95]). Связанная
                                работа предлагает два направления для решения этой проблемы. Во‑первых, можно
                                заимствовать тестовые подходы из исследований по безопасности и приватности языковых
                                моделей, оценивающих вероятность присутствия тестового примера в обучающих данных [19,
                                94]. Один из таких методов — использование canary strings [18]: легко обнаруживаемых
                                синтетических строк, воспроизведение которых явно свидетельствует о запоминании.
                                Во‑вторых, по аналогии с работами по генерации текстовых атак [38] и SeqTrans [28],
                                имена переменных и пользовательских типов можно модифицировать на этапе валидации, когда
                                модель оценивается на способность к обобщению.</p>
                        </div>

                        <div class="subsection">
                            <div class="section-title">Сходство и гранулярность</div>
                            <p>Хотя дублирование на уровне документов, операторов и предложений лучше изучено для
                                языковых моделей в не‑кодовых задачах, в моделях для кода дедупликация чаще всего
                                проводится на уровне файлов [6, 121]. Это может оставлять нетронутыми дублирующиеся
                                фрагменты кода, когда повторение происходит на других уровнях, например на уровне
                                функций. Простая иллюстрация: несколько проектов могут ссылаться на одно и то же решение
                                с StackOverflow, в результате чего в них появляются сходные (адаптированные) функции.
                                Хотя более мелкая сегментация может быть избыточной для задач, таких как суммаризация
                                кода, текущие NPR‑подходы, выполняющие «перевод» относительно небольших фрагментов кода,
                                могут быть особенно уязвимы к дублированию. В нашем корпусе NPR‑техники использовали
                                различные уровни гранулярности входных данных — от контекста, ограниченного максимальной
                                длиной последовательности токенов [28, 68, 129, 131], до целого багованного метода [50];
                                это фактически определяет гранулярность семантических признаков модели.</p>
                        </div>

                        <div class="subsection">
                            <div class="section-title">Влияние near‑duplicates на задачи с кодом</div>
                            <p>Существующие методы дедупликации обычно ищут и удаляют точные совпадения. Этого
                                недостаточно для ML‑подходов к APR, особенно в контексте LLM. Near‑duplicate‑код
                                представляет серьёзную угрозу для языковых моделей [74]. Когда near‑duplicate‑фрагменты
                                присутствуют и в обучении, и в бенчмарках, показатели качества могут завышаться до 100 %
                                по сравнению с результатами на очищенных корпусах [6]. Политики деконтаминации для
                                языковых моделей зачастую используют метрики сходства для естественного языка, а не для
                                кода. К счастью, эту проблему можно решать средствами обнаружения текстовых клонов и
                                метриками сходства кода, традиционно используемыми для выявления плагиата [98] или
                                нарушений лицензий [44]. Хотя в некоторых случаях простые строковые методы превосходят
                                специализированные инструменты при значительных структурных изменениях кода,
                                специализированные техники анализа сходства исходного кода, как правило, эффективнее
                                общих текстовых мер [98].</p>
                        </div>
                    </div>

                    <div class="section">
                        <div class="section-title">Бенчмарки недостаточно стандартизируют оценки</div>

                        <div class="subsection">
                            <div class="section-title">Представление результатов</div>
                            <p>Рассмотрим, как сообщаются результаты экспериментальной оценки при использовании
                                бенчмарка ManyBugs [59] — старой коллекции крупных открытых C‑программ — по сравнению с
                                Defects4J [51], более новой коллекцией небольших открытых Java‑библиотек. Для ManyBugs
                                авторы обычно приводят [79] детализированные результаты по времени работы помимо доли
                                успешно исправленных дефектов. Однако для статей, использующих Defects4J, информация о
                                времени нередко опускается, а внимание сосредотачивается только на факте нахождения (или
                                ненахождения) заплаты. Когда время всё же упоминается, формат отчётности сильно
                                варьирует: могут приводиться лишь верхние границы по «настенным часам» [71] или
                                ограничение по числу запусков тестового набора [50], но без достаточно подробной
                                информации, позволяющей сравнивать эффективность разных работ.</p>
                        </div>

                        <div class="subsection">
                            <div class="section-title">Локализация ошибок</div>
                            <p>Алгоритм, который должен самостоятельно выполнять локализацию ошибки, фактически решает
                                более сложную задачу, чем алгоритм, получающий местоположение дефекта «извне». Важно
                                различать генерацию заплат (задачу генерации кодовых модификаций для последующей оценки)
                                и полный автоматический ремонт (end‑to‑end‑процесс, включающий подзадачи локализации
                                ошибки и выбора приоритета между несколькими решениями). Некоторые недавние работы
                                оценивают производительность как при предположении идеальной локализации, так и без него
                                [34, 82, 120, 138, 139] и явно указывают принятый сценарий. Однако единообразное
                                представление таких результатов пока отсутствует, поэтому при сравнении проектов
                                требуется осторожность.</p>

                            <p>Некоторые авторы используют исключительно информацию, получаемую при запуске тестов и
                                трассировке исполнения, чтобы построить спектральный алгоритм локализации, оценивающий,
                                где вероятнее всего находится дефект [2, 5, 9, 23, 41–43, 45, 46, 54, 55, 60, 65, 68,
                                86, 90, 92, 100, 116, 122, 129, 131]. Другие принимают место, где человек‑разработчик
                                внёс исправление (эта информация обычно доступна в бенчмарке для валидации решения), и
                                фокусируют внимание вокруг этой точки, полностью исключая задачу локализации [26–28, 33,
                                34, 37, 49, 50, 82, 87, 93, 119, 120, 124, 138, 139]. Инструменты NPR обычно
                                предполагают идеальную локализацию, используя имена файлов и номера строк из исправления
                                разработчика для определения местоположения ошибки, вместо того чтобы выводить его в
                                процессе ремонта [137]. Между этими крайностями существует множество вариантов, многие
                                из которых уже реализованы в алгоритмах APR [79].</p>
                        </div>

                        <div class="subsection">
                            <div class="section-title">Корректность</div>
                            <p>Многие статьи используют заместитель для понятия корректности, чаще всего сравнивая
                                сгенерированную заплату с одним эталонным исправлением, предоставленным
                                человеком‑разработчиком [2, 23, 27, 40–42, 46, 48–50, 53, 55, 68, 72, 81, 84, 87, 92,
                                93, 100, 116, 119, 122, 123, 126, 129, 131, 137, 139]. Это ограничивает набор
                                считающихся приемлемыми решений только теми, которые напоминают выбранное человеком
                                исправление, даже если это далеко не единственный (и, возможно, не лучший) способ
                                удовлетворить требованиям к функциональности программы. Поскольку существует бесконечное
                                число способов реализовать любую требуемую функциональность, определять корректность
                                исключительно через произвольный человеческий вариант — чрезвычайно жёсткий критерий.
                            </p>
                        </div>

                        <div class="subsection">
                            <div class="section-title">Бенчмарки APR систематически отличаются от ML‑датасетов</div>
                            <p>Из‑за различных целей и требований к построению бенчмарки APR [51, 59, 69, 78, 99]
                                слишком малы, чтобы на них можно было обучать ML‑инструменты. Они состоят из
                                ограниченного числа пар «баг/исправление» с заплатами, написанными разработчиками, и
                                тестовыми наборами, задающими требования к поведению программы. Небольшое число примеров
                                затрудняет обучение ML‑моделей без привлечения других источников [131]. При этом
                                ML‑датасеты, чтобы быть достаточно крупными, не могут быть столь же тщательно
                                курированы, как APR‑бенчмарки. Например, CrossVul [88] состоит из различий между
                                коммитом, исправляющим уязвимость, и предыдущей версией программы. Однако такие датасеты
                                не включают воспроизводимую среду сборки для оценки заплат, что пока считается ненужным
                                в задачах ML‑генерации заплат [96]. Другие большие датасеты пар «баг–исправление» [50]
                                аналогично не содержат окружений для исполнения и тестовых наборов — по сути, это просто
                                тексты программ.</p>
                        </div>

                        <div class="subsection">
                            <div class="section-title">Доступ к исходному коду, информации о багах и исправлениям</div>
                            <p>Инструменты APR предъявляют требования, влияющие на дизайн бенчмарков. Некоторые средства
                                требуют исторической информации о развитии конкретного проекта: они используют методы
                                информационного поиска (Information Retrieval, IR), анализируя историю коммитов и
                                сообщения к ним. В нашем обзоре только iFixR [56] использует IR‑метод на основе отчётов
                                о багах. Эти IR‑методы возможны в большинстве бенчмарков, поскольку те содержат ссылки
                                на публичные репозитории программ. Полное включение этой истории непосредственно в
                                бенчмарк часто нереализуемо. Исключение — бенчмарки, ориентированные на небольшие или
                                учебные программы, такие как CodeFlaws [107] и IntroClass [59]; здесь истории достаточно
                                короткие, чтобы быть включёнными полностью.</p>
                        </div>

                        <div class="subsection">
                            <div class="section-title">Воспроизводимые сборки программ</div>
                            <p>Хотя ранние инструменты APR всегда полагались на возможность собрать программу из
                                исходников, до появления недавней работы NPR, специально рассматривающей этот аспект для
                                ML‑оценок [96], многие NPR‑подходы избегали вопроса воспроизводимых сборок. Даже при
                                контейнеризации могут возникать конфликтующие требования или зависимости, делающие
                                сборку или запуск некоторых APR‑инструментов несовместимыми с контейнером программы. Это
                                особенно сложно для APR‑средств, которые модифицируют процесс сборки, добавляя
                                собственные требования, как, например, Angelix [79] и Prophet [73].</p>

                            <p>По сравнению со сборками C и C++ такие инструменты, как Gradle и Maven, являются де‑факто
                                стандартами для Java‑программ и жёстко контролируют внешние зависимости. Эти стандартные
                                средства сборки и стабильность Java/JRE делают программы на Java более удобными
                                объектами для APR‑исследований.</p>

                            <p>Наш обзор показывает влияние затрат на сборку по числу статей, использующих ManyBugs [59]
                                — ранний C‑бенчмарк с неполной поддержкой сборки — по сравнению с Defects4J [51],
                                Java‑бенчмарком с полной инфраструктурой на Maven. В нашей выборке только пять статей
                                содержат данные экспериментов на ManyBugs [2, 20, 86, 89, 92]; при этом только две
                                представляют новые результаты, остальные суммируют данные более ранних экспериментов. В
                                то же время 44 статьи содержат результаты на Defects4J [5, 9, 20, 23, 24, 27, 40, 41,
                                45, 46, 48–50, 54–56, 58, 68, 70, 72, 81, 82, 84–86, 100, 108, 109, 112, 116, 117, 119,
                                120, 122, 123, 125, 126, 128–132, 138, 139]. Хотя на различия в популярности этих
                                наборов влияет и то, что они ориентированы на разные языки, некоторые авторы [76] не
                                смогли выполнить ManyBugs и поэтому прибегали к чисто синтаксическому сравнению с
                                человеческим исправлением как приближённой оценке.</p>

                            <p>Несмотря на очевидные недостатки, связанные со сложностью сборки, программы на C и C++
                                составляют значительную долю разрабатываемого ПО [11]. Их распространённость, а также
                                появление кодовых LLM, поддерживающих несколько языков [21], указывают на необходимость
                                единой среды сборки и запуска, поддерживающей широкий спектр APR‑инструментов и языков.
                            </p>
                        </div>

                        <div class="subsection">
                            <div class="section-title">Единообразные меры отчётности</div>
                            <p>Мы обнаружили, что NPR‑подходы ещё не сошлись на стандартных метриках эффективности
                                поиска и зачастую генерируют большое количество непроверенных заплат на каждую
                                багованную программу. В отличие от традиционных APR‑средств, которые по крайней мере
                                гарантируют правдоподобность (plausibility) каждой заплаты, NPR‑техники сначала
                                генерируют кандидатов (через beam search), а затем уже проверяют их правдоподобность и
                                корректность. Мы заметили признаки того, что размер луча (beam size) становится
                                «де‑факто» стандартом, а его выбор оправдывается ссылками на другие работы [26, 49].
                                Значения beam size варьируются от 50 [26, 28] до 200 [131] и до 1000 [49, 93]. Однако
                                beam size — плохая мера эффективности поиска, так как не учитывает размер словаря модели
                                и длину последовательностей. К сожалению, размер словаря не всегда сообщается [49, 68],
                                что затрудняет сравнение эффективности. Если же учесть словарь, NPR‑модели демонстрируют
                                сильно различающиеся эффективные размеры поиска: SeqTrans ( = [1…50], |V| = 8000);
                                VRepair ( = 50, |V| = 5000); RewardRepair ( = [30, 100, 200, 500], |V| = 32 128); CURE (
                                = [1000], |V| = 50 000).</p>
                        </div>
                    </div>

                    <div class="section">
                        <div class="section-title">Обсуждение</div>

                        <div class="subsection">
                            <div class="section-title">Standup4NPR</div>
                            <p>StandUp4NPR [137] был предложен как платформа для стандартизации оценки
                                ML‑ориентированных средств APR. Она определяет чёткий интерфейс для обучения и
                                тестирования и может накладывать ограничения на ресурсы, чтобы облегчить честное
                                сравнение инструментов. Обучающие и тестовые данные берутся из существующих датасетов и
                                бенчмарков, затем обрабатываются и фильтруются с целью минимизировать количество точных
                                дубликатов.</p>

                            <p>Многие аспекты проекта обнадёживают: стандартизация доступа к данным, ресурсов, времени
                                выполнения и критериев оценки создаёт более ровные условия для сопоставления моделей.
                                Однако другие особенности дизайна StandUp4NPR потенциально проблематичны. Ограничивая
                                доступ к данным только предоставленным примерам, мы по сути тестируем лишь то, насколько
                                эффективна данная архитектура в использовании заданного обучающего набора. При этом
                                исключаются другие направления повышения эффективности APR‑инструментов на базе ML:
                                тщательный отбор дополнительных, независимых датасетов; использование вспомогательных
                                входов (история проекта или других проектов), отсутствующих в бенчмарке, и т.п.</p>

                            <p>Схожая проблема заключается в том, что StandUp4NPR разрабатывался для оценки нейросетевых
                                методов и местами неудобен для традиционных APR‑алгоритмов. GenProg [60] и ARJA [133],
                                например, используют информацию об успешности мутации для выбора дальнейших шагов.
                                Однако в StandUp4NPR предполагается, что все заплаты генерируются сразу, а затем
                                независимо оцениваются. Отсутствие цикла с обратной связью делает многие эволюционные
                                поисковые техники несопоставимыми в рамках этой платформы.</p>
                        </div>

                        <div class="subsection">
                            <div class="section-title">APR-COMP 2024</div>
                            <p>Недавно был объявлен первый Международный конкурс по автоматизированному ремонту программ
                                (APR-COMP 2024). В нём предусмотрено четыре трека: функциональные ошибки, уязвимости,
                                студенческие задания и код, сгенерированный ИИ. Для трека функционального ремонта
                                программы выбираются из уже существующих бенчмарков [51, 59, 78, 99]. Это означает, что
                                конкурс наследует все проблемы этих бенчмарков и что их содержимое заранее известно
                                разработчикам моделей. Такой подход отличается от некоторых ежегодных ML‑соревнований,
                                например ImageNet [31], где публиковался открытый обучающий набор, а тестовый оставался
                                закрытым. Тем не менее наличие стандартного API для доступа к нескольким бенчмаркам —
                                значимый плюс для совместимости инструментов ремонта. Как и в StandUp4NPR, единые
                                ограничения по времени и ресурсам выполнения облегчают сравнимость результатов. Наконец,
                                поскольку подача решений происходит через публичный pull request, конкурс фактически
                                требует от участников предоставить исполняемый артефакт — чего зачастую не хватает в
                                публикациях по APR.</p>
                        </div>
                    </div>

                    <div class="section">
                        <div class="section-title">Заключение</div>
                        <p>APR — динамично развивающееся и быстро эволюционирующее направление в инженерии программного
                            обеспечения. Существует уже и экономический эффект от внедрения APR‑техник, о чём
                            свидетельствует возрастающий интерес индустрии. Однако по мере взросления область
                            концентрирует усилия по оценке вокруг нескольких популярных бенчмарков, которые начинают
                            демонстрировать возрастные ограничения. В частности, рост использования ML‑подходов в APR
                            порождает новые проблемы для оценки с помощью существующих бенчмарков.</p>

                        <p>В этой статье мы изложили происхождение ряда таких проблем и предложили способы перенять идеи
                            из других областей для решения общих задач. Утечка данных представляет значительную угрозу
                            достоверности результатов, особенно в контексте языковых моделей с недоступными или
                            нераскрытыми обучающими корпусами. Существенная неоднородность в способах представления
                            результатов делает сравнение разных оценок затруднительным, а подчас и вовсе невозможным.
                            Из‑за проблем, присущих используемым сегодня методам оценки успеха APR‑инструментов,
                            дальнейшее наращивание числа и масштабов эмпирических исследований и сравнений вряд ли
                            прояснит ситуацию, пока эти фундаментальные вопросы не будут решены.</p>
                    </div>

                    <div class="references">
                        <div class="section-title">Литература</div>
                        <div class="reference-list">
                            <div class="reference-item">1. [n. d.]. NeurIPS 2023 Machine Unlearning Challenge.
                                https://unlearning-challenge.github.io/. Accessed: 2023-09-11.</div>
                            <div class="reference-item">2. Afsoon Afzal, ManishMotwani, Kathryn T. Stolee, Yuriy Brun,
                                and Claire Le Goues. 2021. SOSRepair: Expressive Semantic Search for Real-World Program
                                Repair. IEEE Transactions on Software Engineering 47, 10 (Oct. 2021), 2162–2181.</div>
                            <div class="reference-item">3. Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and
                                Kai-Wei Chang. 2021. Unified Pre-training for Program Understanding and Generation.
                                arXiv:2103.06333 [cs]</div>
                            <div class="reference-item">4. ToufiqueAhmed, Noah Rose Ledesma, and Premkumar Devanbu.
                                2023. SynShine: Improved Fixing of Syntax Errors. IEEE Transactions on Software
                                Engineering 49, 4 (April 2023), 2169–2181.</div>
                            <div class="reference-item">5. Aldeida Aleti and Matias Martinez. 2021. E-APR: Mapping the
                                Effectiveness of Automated Program Repair Techniques. Empirical Software Engineering 26,
                                5 (Sept. 2021), 99:1–99:30.</div>
                            <div class="reference-item">6. Miltiadis Allamanis. 2019. The Adverse Effects of Code
                                Duplication inMachine LearningModels of Code. In Proceedings of the 2019 ACMSIGPLAN
                                International Symposium on New Ideas, New Paradigms, and Reflections on Programming and
                                Software (Athens, Greece) (Onward! 2019). Association for Computing Machinery, New York,
                                NY, USA, 143–153. https://doi.org/10.1145/3359591.3359735</div>
                            <div class="reference-item">7. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma,
                                Henryk Michalewski, et al. 2021. Program Synthesis with Large LanguageModels.
                                arXiv:2108.07732 [cs]</div>
                            <div class="reference-item">8. Björn Barz and Joachim Denzler. 2020. Do We Train on Test
                                Data? Purging CIFAR of Near-Duplicates. Journal of Imaging 6, 6 (June 2020), 41.</div>
                            <div class="reference-item">9. Samuel Benton, Xia Li, Yiling Lou, and Lingming Zhang. 2020.
                                On the Effectiveness of Unified Debugging: An Extensive Study on 16 Program Repair
                                Systems. In 35th IEEE/ACM International Conference on Automated Software Engineering,
                                ASE 2020, Melbourne, Australia, September 21-25, 2020. IEEE, 907–918.
                                https://doi.org/10.1145/3324884.3416566</div>
                            <div class="reference-item">10. Sahil Bhatia, Pushmeet Kohli, and Rishabh Singh. 2018.
                                Neuro-Symbolic Program Corrector for Introductory Programming Assignments. In
                                Proceedings of the 40th International Conference on Software Engineering. ACM,
                                Gothenburg Sweden, 60–70.</div>
                            <div class="reference-item">11. Tegawende F. Bissyande, Ferdian Thung, David Lo, Lingxiao
                                Jiang, and Laurent Reveillere. 2013. Popularity, Interoperability, and Impact of
                                Programming Languages in 100,000 Open Source Projects. In 2013 IEEE 37th Annual Computer
                                Software and Applications Conference. IEEE, Kyoto, Japan, 303–312.</div>
                            <div class="reference-item">12. Daniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain,
                                and Lucy Vasserman. 2019. Nuanced Metrics for Measuring Unintended Bias with Real Data
                                for Text Classification. In Companion Proceedings of The 2019 World Wide Web Conference.
                                ACM, San Francisco USA, 491–500.</div>
                            <div class="reference-item">13. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
                                Jared Kaplan, et al. 2020. Language Models Are Few-Shot Learners. In Advances in Neural
                                Information Processing Systems, Vol. 33. NIPS.</div>
                            <div class="reference-item">14. Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan,
                                Johannes Gehrke, Eric Horvitz, et al. 2023. Sparks of Artificial General Intelligence:
                                Early Experiments with GPT-4. arXiv:2303.12712 [cs]</div>
                            <div class="reference-item">15. Yinzhi Cao and Junfeng Yang. 2015. Towards Making Systems
                                Forget with Machine Unlearning. In 2015 IEEE Symposium on Security and Privacy. IEEE,
                                San Jose, CA, 463–480.</div>
                            <div class="reference-item">16. Yinzhi Cao, Alexander Fangxiao Yu, Andrew Aday, Eric Stahl,
                                Jon Merwine, et al. 2018. Efficient Repair of Polluted Machine Learning Systems via
                                Causal Unlearning. In Proceedings of the 2018 on Asia Conference on Computer and
                                Communications Security. ACM, Incheon Republic of Korea, 735–747.</div>
                            <div class="reference-item">17. Nicholas Carlini, Daphne Ippolito, Matthew Jagielski,
                                Katherine Lee, Florian Tramer, et al. 2023. Quantifying Memorization Across Neural
                                Language Models. arXiv:2202.07646 [cs]</div>
                            <div class="reference-item">18. Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos,
                                and Dawn Song. 2019. The Secret Sharer: Evaluating and Testing Unintended Memorization
                                in Neural Networks. In USENIX Security, Vol. 28. USENIX.</div>
                            <div class="reference-item">19. Nicholas Carlini, Florian Tramèr, EricWallace, Matthew
                                Jagielski, Ariel Herbert-Voss, et al. 2021. Extracting Training Data from Large Language
                                Models. In USENIX Security. 2633–2650.</div>
                            <div class="reference-item">20. Padraic Cashin, Carianne Martinez, Westley Weimer, and
                                Stephanie Forrest. 2019. Understanding Automatically-Generated Patches Through Symbolic
                                Invariant Differences. In Automated Software Engineering. IEEE, 411–414.</div>
                            <div class="reference-item">21. Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen,
                                Luna Phipps-Costin, et al. 2023. MultiPL-E: A Scalable and Polyglot Approach to
                                Benchmarking Neural Code Generation. IEEE Trans. Software Eng. 49, 7 (2023), 3675–3691.
                                https://doi.org/10.1109/TSE.2023.3267446</div>
                            <div class="reference-item">22. Sneha Chaudhari, Varun Mithal, Gungor Polatkan, and Rohan
                                Ramanath. 2021. An Attentive Survey of Attention Models. ACM Transactions on Intelligent
                                Systems and Technology 12, 5 (Oct. 2021), 1–32.</div>
                            <div class="reference-item">23. Liushan Chen, Yu Pei, and Carlo A. Furia. 2021.
                                Contract-Based Program Repair Without The Contracts: An Extended Study. IEEE
                                Transactions on Software Engineering 47, 12 (Dec. 2021), 2841–2857.</div>
                            <div class="reference-item">24. Liushan Chen, Yu Pei, Minxue Pan, Tian Zhang, QixinWang, et
                                al. 2023. Program RepairWith Repeated Learning. IEEE Transactions on Software
                                Engineering 49, 2 (Feb. 2023), 831–848.</div>
                            <div class="reference-item">25. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
                                Ponde de Oliveira Pinto, et al. 2021. Evaluating Large Language Models Trained on Code.
                                arXiv:2107.03374 [cs]</div>
                            <div class="reference-item">26. Zimin Chen, Steve Kommrusch, and Martin Monperrus. 2023.
                                Neural Transfer Learning for Repairing Security Vulnerabilities in C Code. IEEE
                                Transactions on Software Engineering 49, 1 (Jan. 2023), 147–165.</div>
                            <div class="reference-item">27. Zimin Chen, Steve Kommrusch, Michele Tufano, Louis-Noel
                                Pouchet, Denys Poshyvanyk, et al. 2019. SEQUENCER: Sequence-to-sequence Learning for
                                End-to-End Program Repair. Transactions on Software Engineering 47, 9 (Sept. 2019),
                                1943–1959.</div>
                            <div class="reference-item">28. Jianlei Chi, Yu Qu, Ting Liu, Qinghua Zheng, and Heng Yin.
                                2023. SeqTrans: Automatic Vulnerability Fix Via Sequence to Sequence Learning. IEEE
                                Transactions on Software Engineering 49, 2 (Feb. 2023), 564–585.</div>
                            <div class="reference-item">29. Kyunghyun Cho, Bart vanMerrienboer, Dzmitry Bahdanau, and
                                Yoshua Bengio. 2014. On the Properties of NeuralMachine Translation: Encoder-Decoder
                                Approaches. arXiv:1409.1259 [cs, stat]</div>
                            <div class="reference-item">30. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
                                Bosma, Gaurav Mishra, et al. 2023. PaLM: Scaling Language Modeling with Pathways.
                                Journal of Machine Learning Research 24, 240 (2023), 1–113.</div>
                            <div class="reference-item">31. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, et
                                al. [n. d.]. ImageNet: A Large-Scale Hierarchical Image Database. ([n. d.]).</div>
                            <div class="reference-item">32. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
                                Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language
                                Understanding. arXiv:1810.04805 [cs]</div>
                            <div class="reference-item">33. Yangruibo Ding, Baishakhi Ray, Premkumar Devanbu, and
                                Vincent J. Hellendoorn. 2020. Patching as Translation: The Data and the Metaphor. In
                                35th IEEE/ACM International Conference on Automated Software Engineering (ASE '20). ACM,
                                Virtual Event.</div>
                            <div class="reference-item">34. Zhiyu Fan, Xiang Gao, Martin Mirchev, Abhik Roychoudhury,
                                and Shin Hwei Tan. 2023. Automated Repair of Programs from Large Language Models. In
                                International Conference on Software Engineering. IEEE, 1469–1481.</div>
                            <div class="reference-item">35. Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng
                                Feng, et al. 2020. CodeBERT: A Pre-Trained Model for Programming and Natural Languages.
                                arXiv:2002.08155 [cs]</div>
                            <div class="reference-item">36. Maik Fröbe, Janek Bevendorff, JanHeinrich Reimer,Martin
                                Potthast, andMatthiasHagen. 2020. Sampling Bias Due to Near-Duplicates in Learning to
                                Rank. In Proceedings of the 43rd International ACM SIGIR Conference on Research and
                                Development in Information Retrieval. ACM, Virtual Event China, 1997–2000.</div>
                            <div class="reference-item">37. Michael Fu, Chakkrit Tantithamthavorn, Trung Le, Van Nguyen,
                                and Dinh Phung. 2022. VulRepair: A T5-basedAutomated Software Vulnerability Repair. In
                                Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium
                                on the Foundations of Software Engineering. ACM, Singapore Singapore, 935–947.</div>
                            <div class="reference-item">38. Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun Qi.
                                2018. Black-box generation of adversarial text sequences to evade deep learning
                                classifiers. In 2018 IEEE Security and Privacy Workshops (SPW). IEEE, 50–56.</div>
                            <div class="reference-item">39. Luca Gazzola, Daniela Micucci, and Leonardo Mariani. 2019.
                                Automatic Software Repair: A Survey. Transactions on Software Engineering 45, 1 (2019),
                                34–67.</div>
                            <div class="reference-item">40. Ali Ghanbari. 2019. Toward Practical Automatic Program
                                Repair. In 34th IEEE/ACM International Conference on Automated Software Engineering, ASE
                                2019, San Diego, CA, USA, November 11-15, 2019. IEEE, 1262–1264.
                                https://doi.org/10.1109/ASE.2019.00156</div>
                            <div class="reference-item">41. Ali Ghanbari and Lingming Zhang. 2019. PraPR: Practical
                                Program Repair via Bytecode Mutation. In 34th IEEE/ACM International Conference on
                                Automated Software Engineering, ASE 2019, San Diego, CA, USA, November 11-15, 2019.
                                IEEE, 1118–1121. https://doi.org/10.1109/ASE.2019.00116</div>
                            <div class="reference-item">42. Davide Ginelli, Matias Martinez, Leonardo Mariani, and
                                Martin Monperrus. 2022. A comprehensive study of code-removal patches in automated
                                program repair. Empir. Softw. Eng. 27, 4 (2022), 97.
                                https://doi.org/10.1007/S10664-021-10100-7</div>
                            <div class="reference-item">43. Matthías Páll Gissurarson, Leonhard Applis, Annibale
                                Panichella, Arie Van Deursen, and David Sands. 2022. PropR: Property-Based Automatic
                                Program Repair. In Proceedings of the 44th International Conference on Software
                                Engineering. ACM, Pittsburgh Pennsylvania, 1768–1780.</div>
                            <div class="reference-item">44. Yaroslav Golubev, Maria Eliseeva, Nikita Povarov, and
                                Timofey Bryksin. 2020. A study of potential code borrowing and license violations in
                                java projects on github. In Proceedings of the 17th International Conference on Mining
                                Software Repositories. 54–64.</div>
                            <div class="reference-item">45. Jinru Hua, Mengshi Zhang, Kaiyuan Wang, and Sarfraz
                                Khurshid. 2018. SketchFix: a tool for automated program repair approach using lazy
                                candidate generation. In Proceedings of the 2018 ACMJointMeeting on European Software
                                Engineering Conference and Symposium on the Foundations of Software Engineering,
                                ESEC/SIGSOFT FSE 2018, Lake Buena Vista, FL, USA, November 04-09, 2018, Gary T. Leavens,
                                Alessandro Garcia, and Corina S. Pasareanu (Eds.). ACM, 888–891.
                                https://doi.org/10.1145/3236024.3264600</div>
                            <div class="reference-item">46. JinruHua,Mengshi Zhang, KaiyuanWang, and Sarfraz Khurshid.
                                2018. Towards Practical ProgramRepairwithOn-Demand Candidate Generation. In Proceedings
                                of the 40th International Conference on Software Engineering. ACM, Gothenburg Sweden,
                                12–23.</div>
                            <div class="reference-item">47. Nikita Jaipuria, Xianling Zhang, Rohan Bhasin, Mayar Arafa,
                                Punarjay Chakravarty, et al. 2020. Deflating Dataset Bias Using Synthetic Data
                                Augmentation. In 2020 IEEE/CVF Conference on Computer Vision and Pattern
                                RecognitionWorkshops (CVPRW). IEEE, Seattle,WA, USA, 3344–3353.</div>
                            <div class="reference-item">48. Nan Jiang, Kevin Liu, Thibaud Lutellier, and Lin Tan. 2023.
                                Impact of Code Language Models on Automated Program Repair. In 45th IEEE/ACM
                                International Conference on Software Engineering, ICSE 2023, Melbourne, Australia, May
                                14-20, 2023. IEEE, 1430–1442. https://doi.org/10.1109/ICSE48619.2023.00125</div>
                            <div class="reference-item">49. Nan Jiang, Thibaud Lutellier, Yiling Lou, Lin Tan, Dan
                                Goldwasser, et al. 2023. KNOD: Domain Knowledge Distilled Tree Decoder for Automated
                                Program Repair. In International Conference on Software Engineering. IEEE.
                                arXiv:2302.01857 [cs]</div>
                            <div class="reference-item">50. Nan Jiang, Thibaud Lutellier, and Lin Tan. 2021. CURE:
                                Code-Aware Neural Machine Translation for Automatic Program Repair. In International
                                Conference on Software Engineering. IEEE, Madrid, Spain, 1161–1173.</div>
                            <div class="reference-item">51. René Just, Darioush Jalali, and Michael D. Ernst. 2014.
                                Defects4J: A Database of Existing Faults to Enable Controlled Testing Studies for Java
                                Programs. In International Symposium on Software Testing and Analysis. ACM, San Jose,
                                CA, USA, 437–440.</div>
                            <div class="reference-item">52. Shachar Kaufman, Saharon Rosset, Claudia Perlich, and Ori
                                Stitelman. 2012. Leakage in Data Mining: Formulation, Detection, and Avoidance. ACM
                                Transactions on Knowledge Discovery from Data 6, 4 (Dec. 2012), 1–21.</div>
                            <div class="reference-item">53. Maria Kechagia, Sergey Mechtaev, Federica Sarro, and Mark
                                Harman. 2022. Evaluating Automatic Program Repair Capabilities to Repair API Misuses.
                                IEEE Transactions on Software Engineering 48, 7 (July 2022), 2658–2679.</div>
                            <div class="reference-item">54. Jindae Kimand Sunghun Kim. 2019. Automatic patch generation
                                with context-based change application. Empir. Softw. Eng. 24, 6 (2019), 4071–4106.
                                https://doi.org/10.1007/S10664-019-09742-5</div>
                            <div class="reference-item">55. Anil Koyuncu, Kui Liu, Tegawendé F. Bissyandé, Dongsun Kim,
                                Jacques Klein, et al. 2020. FixMiner: Mining relevant fix patterns for automated program
                                repair. Empir. Softw. Eng. 25, 3 (2020), 1980–2024.
                                https://doi.org/10.1007/S10664-019-09780-Z</div>
                            <div class="reference-item">56. Anil Koyuncu, Kui Liu, Tegawendé F. Bissyandé, Dongsun Kim,
                                Martin Monperrus, et al. 2019. iFixR: Bug Report Driven Program Repair. In Proceedings
                                of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and
                                Symposium on the Foundations of Software Engineering. ACM, Tallinn Estonia, 314–325.
                            </div>
                            <div class="reference-item">57. Rayson Laroca, Valter Estevam, Alceu S. Britto, Rodrigo
                                Minetto, and David Menotti. 2023. Do We Train on Test Data? The Impact of
                                Near-Duplicates on License Plate Recognition. In 2023 International Joint Conference on
                                Neural Networks (IJCNN). IEEE, Gold Coast, Australia, 1–8.</div>
                            <div class="reference-item">58. Thanh Le-Cong, Duc-Minh Luong, Xuan Bach D. Le, David Lo,
                                Nhat-Hoa Tran, et al. 2023. Invalidator: Automated Patch Correctness Assessment via
                                Semantic and Syntactic Reasoning. IEEE Transactions on Software Engineering (2023),
                                1–20.</div>
                            <div class="reference-item">59. Claire Le Goues, Neal Holtschulte, Edward K. Smith, Yuriy
                                Brun, Premkumar Devanbu, et al. 2015. The ManyBugs and IntroClass Benchmarks for
                                Automated Repair of C Programs. Transactions on Software Engineering 41, 12 (Dec. 2015),
                                1236–1256.</div>
                            <div class="reference-item">60. Claire Le Goues, ThanhVu Nguyen, Stephanie Forrest, and
                                Westley Weimer. 2012. GenProg: A Generic Method for Automatic Software Repair.
                                Transactions on Software Engineering 38, 1 (Jan. 2012), 54–72.</div>
                            <div class="reference-item">61. Claire Le Goues, Michael Pradel, and Abhik Roychoudhury.
                                2019. Automated Program Repair. Commun. ACM 62, 12 (Nov. 2019), 56–65.</div>
                            <div class="reference-item">62. Alexander LeClair, Sakib Haque, LingfeiWu, and Collin
                                McMillan. 2020. Improved Code Summarization via a Graph Neural Network. In Proceedings
                                of the 28th International Conference on Program Comprehension. ACM, Seoul Republic of
                                Korea, 184–195.</div>
                            <div class="reference-item">63. Katherine Lee, Daphne Ippolito, AndrewNystrom,Chiyuan Zhang,
                                Douglas Eck, et al. 2022. Deduplicating TrainingDataMakes LanguageModels Better.
                                arXiv:2107.06499 [cs]</div>
                            <div class="reference-item">64. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
                                Abdelrahman Mohamed, et al. 2020. BART: Denoising Sequence-to-Sequence Pre-training for
                                Natural Language Generation, Translation, and Comprehension. In Proceedings of the 58th
                                Annual Meeting of the Association for Computational Linguistics. Association for
                                Computational Linguistics, Online, 7871–7880.</div>
                            <div class="reference-item">65. Leping Li, Hui Liu, Kejun Li, Yanjie Jiang, and Rui Sun.
                                2023. Generating Concise Patches for Newly Released Programming Assignments. IEEE Trans.
                                Software Eng. 49, 1 (2023), 450–467. https://doi.org/10.1109/TSE.2022.3153522</div>
                            <div class="reference-item">66. Xueyang Li, Shangqing Liu, Ruitao Feng, Guozhu Meng, Xiaofei
                                Xie, et al. 2022. TransRepair: Context-aware Program Repair for Compilation Errors. In
                                Proceedings of the 37th IEEE/ACM International Conference on Automated Software
                                Engineering. ACM, Rochester MI USA, 1–13.</div>
                            <div class="reference-item">67. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian
                                Schrittwieser, et al. 2022. Competition-Level Code Generation with AlphaCode. Science
                                378, 6624 (Dec. 2022), 1092–1097.</div>
                            <div class="reference-item">68. Yi Li, Shaohua Wang, and Tien N. Nguyen. 2020. DLFix:
                                Context-based Code Transformation Learning for Automated Program Repair. In
                                International Conference on Software Engineering. ACM, Seoul, South Korea, 602–614.
                            </div>
                            <div class="reference-item">69. Derrick Lin, James Koppel, Angela Chen, and Armando
                                Solar-Lezama. 2017. Quixbugs: A Multi-Lingual Program Repair Benchmark Set Based on the
                                Quixey Challenge. In Proceedings Companion of the 2017 ACM SIGPLAN International
                                Conference on Systems, Programming, Languages, and Applications: Software for Humanity.
                                ACM, 55–56.</div>
                            <div class="reference-item">70. Kui Liu, Dongsun Kim, Tegawende F. Bissyande, Shin Yoo, and
                                Yves Le Traon. 2021. Mining Fix Patterns for FindBugs Violations. IEEE Transactions on
                                Software Engineering 47, 1 (Jan. 2021), 165–188.</div>
                            <div class="reference-item">71. Kui Liu, Anil Koyuncu, Dongsun Kim, and Tegawendé F.
                                Bissyandé. 2019. TBar: Revisiting Template-based Automated Program Repair. In
                                International Symposium on Software Testing and Analysis. ACM, Beijing, China, 31–42.
                            </div>
                            <div class="reference-item">72. Kui Liu, Shangwen Wang, Anil Koyuncu, Kisub Kim, Tegawendé
                                F. Bissyandé, et al. 2020. On the Efficiency of Test Suite Based Program Repair: A
                                Systematic Assessment of 16 Automated Repair Systems for Java Programs. In Proceedings
                                of the ACM/IEEE 42nd International Conference on Software Engineering. ACM, Seoul South
                                Korea, 615–627.</div>
                            <div class="reference-item">73. Fan Long and Martin Rinard. 2015. Prophet: Automatic Patch
                                Generation via Learning from Successful Patches. Technical Report MIT-CSAIL-TR-2015-027.
                                CSAIL, Massachusetts Institute of Technology. 13 pages.</div>
                            <div class="reference-item">74. Cristina V Lopes, Petr Maj, Pedro Martins, Vaibhav Saini, Di
                                Yang, et al. 2017. DéjàVu: a map of code duplicates on GitHub. Proceedings of the ACM on
                                Programming Languages 1, OOPSLA (2017), 1–28.</div>
                            <div class="reference-item">75. Minh-Thang Luong, Hieu Pham, and Christopher D. Manning.
                                2015. Effective Approaches to Attention-based Neural Machine Translation.
                                arXiv:1508.04025 [cs]</div>
                            <div class="reference-item">76. Thibaud Lutellier, Hung Viet Pham, Lawrence Pang, Yitong Li,
                                Moshi Wei, et al. 2020. CoCoNuT: Combining Context-Aware Neural Translation Models Using
                                Ensemble for Program Repair. In International Symposium on Software Testing and
                                Analysis. ACM, Virtual Event, 101–114.</div>
                            <div class="reference-item">77. AmanMadaan, Shuyan Zhou, Uri Alon, Yiming Yang, and
                                GrahamNeubig. 2022. LanguageModels of Code Are Few-Shot Commonsense Learners.
                                arXiv:2210.07128 [cs]</div>
                            <div class="reference-item">78. Fernanda Madeiral, Simon Urli, MarceloMaia,
                                andMartinMonperrus. 2019. Bears: An Extensible Java Bug Benchmark for Automatic
                                ProgramRepair Studies. In 2019 IEEE 26th International Conference on Software Analysis,
                                Evolution and Reengineering (SANER). 468–478. arXiv:1901.06024 [cs]</div>
                            <div class="reference-item">79. Sergey Mechtaev, Jooyong Yi, and Abhik Roychoudhury. 2016.
                                Angelix: Scalable Multiline Program Patch Synthesis via Symbolic Analysis. In
                                International Conference on Software Engineering. ACM, Austin, TX, USA, 691–701.</div>
                            <div class="reference-item">80. Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina
                                Lerman, and Aram Galstyan. 2022. A Survey on Bias and Fairness in Machine Learning.
                                Comput. Surveys 54, 6 (July 2022), 1–35.</div>
                            <div class="reference-item">81. Xiangxin Meng, Xu Wang, Hongyu Zhang, Hailong Sun, and
                                Xudong Liu. 2022. Improving Fault Localization and Program Repair with Deep Semantic
                                Features and Transferred Knowledge. In Proceedings of the 44th International Conference
                                on Software Engineering. ACM, Pittsburgh Pennsylvania, 1169–1180.</div>
                            <div class="reference-item">82. Xiangxin Meng, XuWang, Hongyu Zhang, Hailong Sun, Xudong
                                Liu, et al. 2023. Template-Based Neural Program Repair. In 2023 IEEE/ACM45th
                                International Conference on Software Engineering (ICSE). IEEE, Melbourne, Australia,
                                1456–1468.</div>
                            <div class="reference-item">83. Martin Monperrus. [n. d.]. The Living Review on Automated
                                Program Repair. ([n. d.]).</div>
                            <div class="reference-item">84. Manish Motwani and Yuriy Brun. 2023. Better Automatic
                                Program Repair by Using Bug Reports and Tests Together. In 45th IEEE/ACM International
                                Conference on Software Engineering, ICSE 2023, Melbourne, Australia, May 14-20, 2023.
                                IEEE, 1225–1237. https://doi.org/10.1109/ICSE48619.2023.00109</div>
                            <div class="reference-item">85. Manish Motwani, Sandhya Sankaranarayanan, René Just, and
                                Yuriy Brun. 2018. Do automated program repair techniques repair hard and important bugs?
                                Empir. Softw. Eng. 23, 5 (2018), 2901–2947. https://doi.org/10.1007/S10664-017-9550-0
                            </div>
                            <div class="reference-item">86. Manish Motwani, Mauricio Soto, Yuriy Brun, Rene Just, and
                                Claire Le Goues. 2022. Quality of Automated Program Repair on Real-World Defects. IEEE
                                Transactions on Software Engineering 48, 2 (Feb. 2022), 637–661.</div>
                            <div class="reference-item">87. Marjane Namavar, Noor Nashid, and Ali Mesbah. 2022. A
                                Controlled Experiment of Different Code Representations for Learning-Based Program
                                Repair. Empirical Software Engineering 27, 7 (Dec. 2022), 190.</div>
                            <div class="reference-item">88. Georgios Nikitopoulos, Konstantina Dritsa, Panos Louridas,
                                and Dimitris Mitropoulos. 2021. CrossVul: a cross-language vulnerability dataset with
                                commit data. In Proceedings of the 29th ACM Joint Meeting on European Software
                                Engineering Conference and Symposium on the Foundations of Software Engineering.
                                1565–1569.</div>
                            <div class="reference-item">89. Yannic Noller, Ridwan Shariffdeen, Xiang Gao, and Abhik
                                Roychoudhury. 2022. Trust Enhancement Issues in Program Repair. In Proceedings of the
                                44th International Conference on Software Engineering. ACM, Pittsburgh Pennsylvania,
                                2228–2240.</div>
                            <div class="reference-item">90. Wonseok Oh and Hakjoo Oh. 2022. PyTER: effective program
                                repair for Python type errors. In Proceedings of the 30th ACM Joint European Software
                                Engineering Conference and Symposium on the Foundations of Software Engineering,
                                ESEC/FSE 2022, Singapore, Singapore, November 14-18, 2022, Abhik Roychoudhury, Cristian
                                Cadar, and Miryung Kim (Eds.). ACM, 922–934. https://doi.org/10.1145/3540250.3549130
                            </div>
                            <div class="reference-item">91. OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774
                                [cs.CL]</div>
                            <div class="reference-item">92. Nikhil Parasaram, Earl T. Barr, and Sergey Mechtaev. 2021.
                                Trident: Controlling Side Effects in Automated Program Repair. IEEE Transactions on
                                Software Engineering (2021), 1–1.</div>
                            <div class="reference-item">93. Nikhil Parasaram, Earl T. Barr, and SergeyMechtaev. 2023.
                                Rete: Learning Namespace Representation for Program Repair. In 2023 IEEE/ACM45th
                                International Conference on Software Engineering (ICSE). IEEE, Melbourne, Australia,
                                1264–1276.</div>
                            <div class="reference-item">94. Charith Peris, Christophe Dupuy, Jimit Majmudar, Rahil
                                Parikh, Sami Smaili, et al. 2023. Privacy in the Time of Language Models. In Proceedings
                                of the Sixteenth ACM International Conference on Web Search and Data Mining. 1291–1292.
                            </div>
                            <div class="reference-item">95. Jason Phang, Herbie Bradley, Leo Gao, Louis Castricato, and
                                Stella Biderman. 2022. EleutherAI: Going Beyond" Open Science" to" Science in the Open".
                                arXiv preprint arXiv:2210.06413 (2022).</div>
                            <div class="reference-item">96. Julian Aron Prenner and Romain Robbes. 2023. RunBugRun – An
                                Executable Dataset for Automated Program Repair. arXiv:2304.01102 [cs.SE]</div>
                            <div class="reference-item">97. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
                                Sharan Narang, et al. 2020. Exploring the Limits of Transfer Learning with a Unified
                                Text-to-Text Transformer. Journal of Machine Learning Research 21, 1 (2020), 5485–5551.
                            </div>
                            <div class="reference-item">98. Chaiyong Ragkhitwetsagul, Jens Krinke, and David Clark.
                                2018. A comparison of code similarity analysers. Empirical Software Engineering 23, 4
                                (Aug 2018), 2464–2519. https://doi.org/10.1007/s10664-017-9564-7</div>
                            <div class="reference-item">99. Ripon K. Saha, Yingjun Lyu,Wing Lam, Hiroaki Yoshida, and
                                Mukul R. Prasad. 2018. Bugs.Jar: A Large-Scale, Diverse Dataset of Real-World Java Bugs.
                                In Proceedings of the 15th International Conference on Mining Software Repositories -
                                MSR '18. ACM Press, Gothenburg, Sweden, 10–13.</div>
                            <div class="reference-item">100. Seemanta Saha, Ripon K. Saha, and Mukul R. Prasad. 2019.
                                Harnessing Evolution for Multi-Hunk Program Repair. In International Conference on
                                Software Engineering. IEEE, Montreal, QC, Canada, 13–24.</div>
                            <div class="reference-item">101. Teven Le Scao, Angela Fan, Christopher Akiki, Ellie
                                Pavlick, Suzana Ilić, et al. 2023. BLOOM: A 176B-Parameter Open-Access Multilingual
                                Language Model. arXiv:2211.05100 [cs]</div>
                            <div class="reference-item">102. Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. 2023.
                                Are Emergent Abilities of Large Language Models a Mirage? arXiv:2304.15004 [cs]</div>
                            <div class="reference-item">103. M. Schuster and K.K. Paliwal. Nov./1997. Bidirectional
                                Recurrent Neural Networks. IEEE Transactions on Signal Processing 45, 11 (Nov./1997),
                                2673–2681.</div>
                            <div class="reference-item">104. Virat Shejwalkar and Amir Houmansadr. 2021. Membership
                                Privacy for Machine Learning Models Through Knowledge Transfer. Proceedings of the AAAI
                                Conference on Artificial Intelligence 35, 11 (May 2021), 9549–9557.</div>
                            <div class="reference-item">105. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. [n. d.].
                                Sequence to Sequence Learning with Neural Networks. ([n. d.]).</div>
                            <div class="reference-item">106. Alexandre Tamborrino, Nicola Pellicano, Baptiste Pannier,
                                Pascal Voitot, and Louise Naudin. 2020. Pre-Training Is (Almost) All You Need: An
                                Application to Commonsense Reasoning. arXiv:2004.14074 [cs]</div>
                            <div class="reference-item">107. Shin Hwei Tan, Jooyong Yi, SergeyMechtaev, Abhik
                                Roychoudhury, et al. 2017. Codeflaws: A Programming Competition Benchmark for Evaluating
                                Automated ProgramRepair Tools. In Proceedings of the 39th International Conference on
                                Software Engineering Companion. IEEE Press, 180–182.</div>
                            <div class="reference-item">108. Haoye Tian, Kui Liu, Abdoul Kader Kaboré, Anil Koyuncu, Li
                                Li, et al. 2020. Evaluating Representation Learning of Code Changes for Predicting Patch
                                Correctness in Program Repair. In Proceedings of the 35th IEEE/ACM International
                                Conference on Automated Software Engineering. ACM, Virtual Event Australia, 981–992.
                            </div>
                            <div class="reference-item">109. Haoye Tian, Xunzhu Tang, Andrew Habib, Shangwen Wang, Kui
                                Liu, et al. 2022. Is This Change the Answer to That Problem?: Correlating Descriptions
                                of Bug and Code Changes for Evaluating Patch Correctness. In Proceedings of the 37th
                                IEEE/ACMInternational Conference on Automated Software Engineering. ACM, Rochester MI
                                USA, 1–13.</div>
                            <div class="reference-item">110. Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano
                                Di Penta, Martin White, et al. 2019. An Empirical Study on Learning Bug-Fixing Patches
                                in the Wild via Neural Machine Translation. ACM Transactions on Software Engineering and
                                Methodology 28, 4 (Oct. 2019), 1–29.</div>
                            <div class="reference-item">111. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
                                Llion Jones, et al. 2017. Attention Is All You Need. Advances in Neural Information
                                Processing Systems 30 (2017).</div>
                            <div class="reference-item">112. Shangwen Wang, Ming Wen, Bo Lin, Hongjun Wu, Yihao Qin, et
                                al. 2020. Automated Patch Correctness Assessment: How Far Are We?. In Automated Software
                                Engineering. ACM, Virtual Event, 968–980.</div>
                            <div class="reference-item">113. ThomasWang, AdamRoberts, DanielHesslow, Teven Le Scao,
                                HyungWon Chung, et al. 2022. What LanguageModel Architecture and Pretraining
                                ObjectiveWork Best for Zero-Shot Generalization?. In International Conference on Machine
                                Learning. 22964–22984.</div>
                            <div class="reference-item">114. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma,
                                Brian Ichter, et al. [n. d.]. Chain-of-Thought Prompting Elicits Reasoning in Large
                                Language Models. ([n. d.]).</div>
                            <div class="reference-item">115. Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson,
                                et al. 2023. Larger Language Models Do In-Context Learning Differently. arXiv:2303.03846
                                [cs]</div>
                            <div class="reference-item">116. Ming Wen, Junjie Chen, Rongxin Wu, Dan Hao, and Shing-Chi
                                Cheung. 2018. Context-Aware Patch Generation for Better Automated Program Repair. In
                                Proceedings of the 40th International Conference on Software Engineering. ACM,
                                Gothenburg Sweden, 1–11.</div>
                            <div class="reference-item">117. Chu-PanWong, Priscila Santiesteban, Christian Kästner, and
                                Claire Le Goues. 2021. VarFix: balancing edit expressiveness and search effectiveness in
                                automated program repair. In ESEC/FSE '21: 29th ACM Joint European Software Engineering
                                Conference and Symposium on the Foundations of Software Engineering, Athens, Greece,
                                August 23-28, 2021, Diomidis Spinellis, Georgios Gousios, Marsha Chechik,
                                andMassimiliano Di Penta (Eds.). ACM, 354–366. https://doi.org/10.1145/3468264.3468600
                            </div>
                            <div class="reference-item">118. Ga Wu, Masoud Hashemi, and Christopher Srinivasa. 2022.
                                PUMA: Performance Unchanged Model Augmentation for Training Data Removal. Proceedings of
                                the AAAI Conference on Artificial Intelligence 36, 8 (June 2022), 8675–8682.</div>
                            <div class="reference-item">119. Chunqiu Steven Xia, Yuxiang Wei, and Lingming Zhang. 2023.
                                Automated Program Repair in the Era of Large Pre-trained Language Models. In 2023
                                IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE, Melbourne,
                                Australia, 1482–1494.</div>
                            <div class="reference-item">120. Chunqiu Steven Xia and Lingming Zhang. 2022. Less Training,
                                More Repairing Please: Revisiting Automated Program Repair via Zero-Shot Learning. In
                                Proceedings of the 30thACMJoint European Software Engineering Conference and Symposiumon
                                the Foundations of Software Engineering. ACM, Singapore Singapore, 959–971.</div>
                            <div class="reference-item">121. Frank F Xu, Uri Alon, Graham Neubig, and Vincent Josua
                                Hellendoorn. 2022. A systematic evaluation of large language models of code. In
                                Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming. 1–10.
                            </div>
                            <div class="reference-item">122. Tongtong Xu, Liushan Chen, Yu Pei, Tian Zhang,Minxue Pan,
                                et al. 2022. Restore: Retrospective Fault Localization Enhancing Automated Program
                                Repair. IEEE Transactions on Software Engineering 48, 1 (Jan. 2022), 309–326.</div>
                            <div class="reference-item">123. Xuezheng Xu, Yulei Sui, Hua Yan, and Jingling Xue. 2019.
                                VFix: Value-Flow-Guided Precise ProgramRepair for Null Pointer Dereferences. In 2019
                                IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEE, Montreal,
                                QC, Canada, 512–523.</div>
                            <div class="reference-item">124. Chen Yang. 2021. Accelerating Redundancy-Based
                                ProgramRepair via Code Representation Learning and Adaptive Patch Filtering. In
                                Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference
                                and Symposium on the Foundations of Software Engineering. ACM, Athens Greece, 1672–1674.
                            </div>
                            <div class="reference-item">125. Deheng Yang, Kui Liu, Dongsun Kim, Anil Koyuncu, Kisub Kim,
                                et al. 2021. Where were the repair ingredients for Defects4j bugs? Empir. Softw. Eng.
                                26, 6 (2021), 122. https://doi.org/10.1007/S10664-021-10003-7</div>
                            <div class="reference-item">126. Deheng Yang, XiaoguangMao, Liqian Chen, Xuezheng Xu, Yan
                                Lei, et al. 2022. TransplantFix: Graph Differencing-based Code Transplantation for
                                Automated Program Repair. In 37th IEEE/ACM International Conference on Automated
                                Software Engineering, ASE 2022, Rochester, MI, USA, October 10-14, 2022. ACM,
                                107:1–107:13. https://doi.org/10.1145/3551349.3556893</div>
                            <div class="reference-item">127. He Ye, Jian Gu, Matias Martinez, Thomas Durieux, and Martin
                                Monperrus. 2021. Automated Classification of Overfitting Patches with Statically
                                Extracted Code Features. Transactions on Software Engineering Early Access (April 2021),
                                1–20.</div>
                            <div class="reference-item">128. He Ye, Jian Gu, Matias Martinez, Thomas Durieux, and Martin
                                Monperrus. 2022. Automated Classification of Overfitting Patches With Statically
                                Extracted Code Features. IEEE Transactions on Software Engineering 48, 8 (Aug. 2022),
                                2920–2938.</div>
                            <div class="reference-item">129. He Ye, Matias Martinez, Xiapu Luo, Tao Zhang, and Martin
                                Monperrus. 2022. SelfAPR: Self-supervised Program Repair with Test Execution
                                Diagnostics. In Automated Software Engineering. ACM, Oakland Center, MI, US, 1–13.</div>
                            <div class="reference-item">130. He Ye, Matias Martinez, and Martin Monperrus. 2021.
                                Automated patch assessment for program repair at scale. Empir. Softw. Eng. 26, 2 (2021),
                                20. https://doi.org/10.1007/S10664-020-09920-W</div>
                            <div class="reference-item">131. He Ye, MatiasMartinez, and MartinMonperrus. 2022. Neural
                                Program Repair with Execution-Based Backpropagation. In International Conference on
                                Software Engineering. ACM, Pittsburgh, PA, US, 1506–1518.</div>
                            <div class="reference-item">132. Zhongxing Yu, Matias Martinez, Benjamin Danglot, Thomas
                                Durieux, and Martin Monperrus. 2019. Alleviating patch overfitting with automatic test
                                generation: a study of feasibility and effectiveness for the Nopol repair system. Empir.
                                Softw. Eng. 24, 1 (2019), 33–67. https://doi.org/10.1007/S10664-018-9619-4</div>
                            <div class="reference-item">133. Yuan Yuan and Wolfgang Banzhaf. 2020. ARJA: Automated
                                Repair of Java Programs via Multi-objective Genetic Programming. Transactions on
                                Software Engineering 46, 10 (2020), 1040–1067.</div>
                            <div class="reference-item">134. Haibo Zhang, Toru Nakamura, Takamasa Isohara, and Kouichi
                                Sakurai. 2023. A Review on Machine Unlearning. SN Computer Science 4, 4 (April 2023),
                                337.</div>
                            <div class="reference-item">135. Jiyang Zhang, Sheena Panthaplackel, Pengyu Nie, Junyi Jessy
                                Li, and Milos Gligoric. 2022. CoditT5: Pretraining for Source Code and Natural Language
                                Editing. In Proceedings of the 37th IEEE/ACM International Conference on Automated
                                Software Engineering. ACM, Rochester MI USA, 1–12.</div>
                            <div class="reference-item">136. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei
                                Wang, et al. 2023. A Survey of Large Language Models. arXiv:2303.18223 [cs]</div>
                            <div class="reference-item">137. Wenkang Zhong, Hongliang Ge, Hongfei Ai, Chuanyi Li, Kui
                                Liu, et al. 2022. StandUp4NPR: Standardizing SetUp for Empirically Comparing Neural
                                ProgramRepair Systems. In Proceedings of the 37th IEEE/ACMInternational Conference on
                                Automated Software Engineering. ACM, Rochester MI USA, 1–13.</div>
                            <div class="reference-item">138. Qihao Zhu, Zeyu Sun, Yuan-an Xiao, Wenjie Zhang, Kang Yuan,
                                et al. 2021. A Syntax-Guided Edit Decoder for Neural Program Repair. In Proceedings of
                                the 29th ACMJoint Meeting on European Software Engineering Conference and Symposium on
                                the Foundations of Software Engineering. ACM, Athens Greece, 341–353.</div>
                            <div class="reference-item">139. Qihao Zhu, Zeyu Sun, Wenjie Zhang, Yingfei Xiong, and Lu
                                Zhang. 2023. Tare: Type-Aware Neural Program Repair. In 2023 IEEE/ACM 45th International
                                Conference on Software Engineering (ICSE). IEEE, Melbourne, Australia, 1443–1455.</div>
                        </div>
                    </div>

                    <a href="../../science/index_ru.html" class="article-back-link">← Назад к научным трудам</a>
                </div>
            </div>
        </div>
    </div>

    <!-- Нижнее меню -->
    <footer class="footer-menu">
        <a class="footer-item" href="../../index_ru.html">Резюме</a>
        <a class="footer-item" href="../../diss/index_ru.html">Реферат</a>
        <a class="footer-item" href="../../science/index_ru.html">Научные труды</a>
        <a class="footer-item" href="../../ind/index_ru.html">Индивидуальный раздел</a>
    </footer>

    <script src="../../js/particles-config.js"></script>
    <script src="../../js/main.js"></script>
    <script src="../../js/script.js"></script>

</body>

</html>